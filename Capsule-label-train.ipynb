{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **模型训练代码**\n",
    "\n",
    "#### 数据内容存在于./Data中，模型保存在./Model_save中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "import copy\n",
    "import utils\n",
    "import time\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import *\n",
    "from keras import Model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from model import Capsule\n",
    "\n",
    "# 关闭警告信息\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##########################################################\n",
    "\n",
    "label_to_ix=np.load('Data/label_to_ix.npy',allow_pickle=True).item()\n",
    "training_data=np.load('Data/training_data.npy',allow_pickle=True)\n",
    "test_data=np.load('Data/test_data.npy',allow_pickle=True)\n",
    "val_data=np.load('Data/val_data.npy',allow_pickle=True)\n",
    "word_to_ix=np.load('Data/word_to_ix.npy',allow_pickle=True).item()\n",
    "label_emb_weight = np.load('Data/label_embedding.npy',allow_pickle=True) # 标签信息获取在'标签引入预处理.ipynb'\n",
    "embed_weight = utils.build_embeedding_news(\"../../emb/PubMed-and-PMC-w2v.bin\",word_to_ix,\"Data/.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置超参数\n",
    "\n",
    "其中EMB_DIM为embedding尺寸，不能修改。\n",
    "\n",
    "可调参参数：BATCH_SIZE、HIDDEN_DIM、N_CAP、CAP_DIM、ROUTINGS、LABEL_EMBEDDING_TRAIN、WORD_EMBEDDING_TRAIN；\n",
    "\n",
    "+ 其中N_CAP、CAP_DIM、ROUTINGS为胶囊网络参数，代码见./model.py；\n",
    "+ LABEL_EMBEDDING_TRAIN表示训练中更新label的embedding；\n",
    "+ WORD_EMBEDDING_TRAIN表示训练时更新word的embedding；\n",
    "\n",
    "本调参结果较好，建议备份后再调参。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数区域\n",
    "PATIENT = 5\n",
    "MIN_EPOCH = 10\n",
    "MAX_EPOCH = 100\n",
    "BATCH_SIZE = 16\n",
    "SAVE_MODEL = 'Model_save/best_model_capsule_label.h5'\n",
    "LABEL_NUM = len(label_to_ix.keys())\n",
    "EMB_DIM = 200\n",
    "HIDDEN_DIM = 300\n",
    "N_CAP = 50\n",
    "CAP_DIM = 50\n",
    "ROUTINGS = 3\n",
    "LABEL_EMBEDDING_TRAIN = True\n",
    "WORD_EMBEDDING_TRAIN = False\n",
    "\n",
    "def set_seed(THATSEED):\n",
    "    np.random.seed(THATSEED)\n",
    "    tf.compat.v1.set_random_seed(THATSEED)\n",
    "set_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = utils.preprocessing(training_data,label_to_ix,BATCH_SIZE,word_to_ix)\n",
    "batch_val = utils.preprocessing(val_data,label_to_ix,BATCH_SIZE,word_to_ix)\n",
    "batch_test = utils.preprocessing(test_data,label_to_ix,BATCH_SIZE,word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **构造模型部分**\n",
    "\n",
    "+ 首先将label的embedding与每个word的embedding求cos相似度，然后将相似度与word embedding连接，送入BiLSTM；\n",
    "+ 然后利用胶囊网络进行特征提取，输出[N_CAP,CAP_DIM]的特征，然后划归平面为2500维特征，进行EMB_DIM的编码句子后与label embedding求cos相似度\n",
    "+ 最后将相似度直接加到分类效果中；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整体模型部分\n",
    "def build_model(vocab,n_cap,cap_dim,n_class):\n",
    "    word_input = Input(shape=(None,), dtype=\"int32\")\n",
    "    label_input = Input(shape=(LABEL_NUM,),dtype=\"int32\")\n",
    "    \n",
    "    label_emb = Embedding(LABEL_NUM,EMB_DIM,weights=[label_emb_weight],trainable=LABEL_EMBEDDING_TRAIN)\n",
    "    embed = Embedding(len(vocab) + 1,\n",
    "                        EMB_DIM,\n",
    "                        weights=[embed_weight],\n",
    "                        trainable=WORD_EMBEDDING_TRAIN,\n",
    "                        )\n",
    "    \n",
    "    word_embed = embed(word_input)\n",
    "    label_embed = label_emb(label_input)\n",
    "    \n",
    "    # 计算每个单词和每个标签的cos相似度，然后缀加到单词embedding后\n",
    "    word_label_embed = dot([word_embed,label_embed],axes=(2,2),normalize=True)\n",
    "    word_embed = concatenate([word_embed,word_label_embed])\n",
    "    \n",
    "    # 利用capsule的特征提取\n",
    "    x = Bidirectional(CuDNNLSTM(HIDDEN_DIM,return_sequences=True))(word_embed)\n",
    "    x = Capsule(\n",
    "        num_capsule=n_cap,dim_capsule=cap_dim,\n",
    "        routings=ROUTINGS, share_weights=True)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # 利用50*50的特征进行编码文档，然后与标签的embedding求cos相似度\n",
    "    x_ = Dense(EMB_DIM,activation='relu')(x)\n",
    "    x_ = Dropout(0.2)(x_) \n",
    "    label_x = dot([x_,label_embed],axes=(1,2),normalize=True)\n",
    "    \n",
    "    outputs = Dense(n_class)(x)\n",
    "    \n",
    "    # 将分类器的结果与标签文本相似度概率累加\n",
    "    outputs = add([outputs,label_x])\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    \n",
    "    model = Model(inputs=(word_input,label_input), outputs=outputs)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    # print(model.summary())\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = build_model(word_to_ix,N_CAP,CAP_DIM,LABEL_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练过程\n",
    "\n",
    "选取micro_f1最好的epoch作为最终选择的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The epoch spend time 0.0 s.\n",
      "******************** 1 ********************\n",
      "2019-12-16 05:13:53 Epoch 1/100 training\n",
      "train loss:  0.04778787  , train acc:  0.98593354\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.04961002516898733\n",
      "micro F1 0.4790391176333131\n",
      "loss 0.04048001003596015\n",
      "The epoch spend time 871.2 s.\n",
      "******************** 2 ********************\n",
      "2019-12-16 05:28:25 Epoch 2/100 training\n",
      "train loss:  0.03390002  , train acc:  0.9893054\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.11933904332628274\n",
      "micro F1 0.6116720432359966\n",
      "loss 0.03160148017153733\n",
      "The epoch spend time 447.0 s.\n",
      "******************** 3 ********************\n",
      "2019-12-16 05:35:52 Epoch 3/100 training\n",
      "train loss:  0.029481605  , train acc:  0.99031365\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.17495324655186362\n",
      "micro F1 0.6530788225610473\n",
      "loss 0.029815750530606706\n",
      "The epoch spend time 447.4 s.\n",
      "******************** 4 ********************\n",
      "2019-12-16 05:43:19 Epoch 4/100 training\n",
      "train loss:  0.0270852  , train acc:  0.99089175\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.20919027249656452\n",
      "micro F1 0.6637464470661527\n",
      "loss 0.028692079031426676\n",
      "The epoch spend time 447.4 s.\n",
      "******************** 5 ********************\n",
      "2019-12-16 05:50:46 Epoch 5/100 training\n",
      "train loss:  0.025254415  , train acc:  0.9913551\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.2324926704597777\n",
      "micro F1 0.6705656722936407\n",
      "loss 0.028796821860964536\n",
      "The epoch spend time 448.3 s.\n",
      "******************** 6 ********************\n",
      "2019-12-16 05:58:15 Epoch 6/100 training\n",
      "train loss:  0.023591524  , train acc:  0.9918251\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.25000967181269795\n",
      "micro F1 0.673737144585602\n",
      "loss 0.028563740162184534\n",
      "The epoch spend time 447.6 s.\n",
      "******************** 7 ********************\n",
      "2019-12-16 06:05:42 Epoch 7/100 training\n",
      "train loss:  0.021904794  , train acc:  0.9923247\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.2595821026484087\n",
      "micro F1 0.6752473021582733\n",
      "loss 0.02882697336137929\n",
      "The epoch spend time 447.3 s.\n",
      "******************** 8 ********************\n",
      "2019-12-16 06:13:10 Epoch 8/100 training\n",
      "train loss:  0.0202296  , train acc:  0.99280095\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.2553713306731246\n",
      "micro F1 0.6740331491712708\n",
      "loss 0.02934073156034602\n",
      "The epoch spend time 446.0 s.\n",
      "******************** 9 ********************\n",
      "2019-12-16 06:20:36 Epoch 9/100 training\n",
      "train loss:  0.018502284  , train acc:  0.99335355\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.26184590449459044\n",
      "micro F1 0.6684707031984064\n",
      "loss 0.03016290179309421\n",
      "The epoch spend time 446.0 s.\n",
      "******************** 10 ********************\n",
      "2019-12-16 06:28:02 Epoch 10/100 training\n",
      "train loss:  0.016781881  , train acc:  0.9939254\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.264616808320513\n",
      "micro F1 0.6626884776838656\n",
      "loss 0.031228752733182763\n",
      "The epoch spend time 446.4 s.\n",
      "******************** 11 ********************\n",
      "2019-12-16 06:35:28 Epoch 11/100 training\n",
      "train loss:  0.0151482355  , train acc:  0.99445254\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.2611434647241439\n",
      "micro F1 0.6613404183662014\n",
      "loss 0.03223091867217358\n",
      "The epoch spend time 446.7 s.\n",
      "******************** 12 ********************\n",
      "2019-12-16 06:42:55 Epoch 12/100 training\n",
      "train loss:  0.013568985  , train acc:  0.9950219\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.260543217599771\n",
      "micro F1 0.6584106943928705\n",
      "loss 0.03370573172645819\n",
      "The epoch spend time 445.8 s.\n",
      "******************** 13 ********************\n",
      "2019-12-16 06:50:21 Epoch 13/100 training\n",
      "train loss:  0.012113896  , train acc:  0.995539\n",
      "(5264, 344) (5264, 344)\n",
      "macro F1 0.2549454653799121\n",
      "micro F1 0.6532815677574414\n",
      "loss 0.0349345082264269\n",
      "The best scores is  0.6752473021582733\n"
     ]
    }
   ],
   "source": [
    "# 训练过程\n",
    "bound = 0\n",
    "best_scores = -10\n",
    "leasttime = time.time()\n",
    "label_list =  np.expand_dims(np.arange(LABEL_NUM), axis=0)\n",
    "label_list = np.repeat(label_list,BATCH_SIZE,axis=0)\n",
    "for epoch in range(1, MAX_EPOCH + 1):\n",
    "    #break\n",
    "    epoch_train = []\n",
    "    epoch_val = []\n",
    "    print(\"The epoch spend time %.1f s.\" % (time.time()-leasttime))\n",
    "    print(\"*\"*20,epoch,\"*\"*20)\n",
    "    leasttime = time.time()\n",
    "    # print(\"-\"*20)\n",
    "    print('{} Epoch {}/{} training'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()),epoch, MAX_EPOCH))\n",
    "    for i,batch_data in enumerate(batch_train):\n",
    "        item = model.train_on_batch([batch_data[0],label_list], batch_data[2])\n",
    "        epoch_train.append(item)\n",
    "    train_loss, train_acc = np.mean(epoch_train, axis=0)\n",
    "    print(\"train loss: \", train_loss, \" , train acc: \", train_acc)\n",
    "\n",
    "    # 验证集验证效果\n",
    "    y_pred = [] #model.predict(val_x1)\n",
    "    y_val = []\n",
    "    for batch_data in batch_val:\n",
    "        epoch_val.append(model.evaluate([batch_data[0],label_list], batch_data[2],verbose=0))\n",
    "        y_pred.append(model.predict([batch_data[0],label_list]))\n",
    "        y_val.append(batch_data[2])\n",
    "    \n",
    "    val_loss,val_acc = np.mean(epoch_val, axis=0)\n",
    "    y_pred = np.concatenate(y_pred,axis=0)\n",
    "    y_val = np.concatenate(y_val,axis=0)\n",
    "    y_pred = np.array([[1 if i >= 0.5 else 0 for i in t] for t in y_pred])\n",
    "    \n",
    "    y_true_label = y_val\n",
    "    print(y_pred.shape,y_true_label.shape)\n",
    "    micro_f1 = f1_score(y_true_label, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true_label, y_pred, average='macro')\n",
    "    print('macro F1', f1_score(y_true_label, y_pred, average='macro'))\n",
    "    print('micro F1', f1_score(y_true_label, y_pred, average='micro'))\n",
    "    print('loss',val_loss)\n",
    "    scores = micro_f1 # 将验证集loss作为择优标准\n",
    "    # 每个epoch判断是否上升\n",
    "    if scores > best_scores:\n",
    "        best_scores = scores\n",
    "        bound = 0\n",
    "        # with open(os.path.join(OUTPUT_PATH,'logs.log'),'a+') as f:\n",
    "        #     f.write(\"Epoch %.d : macro_f1 = %.4f, micro_f1 = %.4f \\n\" % (epoch,macro_f1,micro_f1))\n",
    "        model.save(SAVE_MODEL)\n",
    "    else:\n",
    "        bound += 1\n",
    "    if bound > PATIENT and epoch > MIN_EPOCH:\n",
    "        print(\"The best scores is \",best_scores)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** test ********************\n",
      "macro F1: 0.2645\n",
      "micro F1: 0.6753\n",
      "Saved model: Model_save/best_model_capsule_label-0.6753.h5\n"
     ]
    }
   ],
   "source": [
    "# 测试过程\n",
    "print(\"*\"*20,\"test\",\"*\"*20)\n",
    "model = load_model(SAVE_MODEL,custom_objects={'Capsule':Capsule})\n",
    "test_y_pred = []\n",
    "test_y = []\n",
    "for batch_data in batch_test:\n",
    "    test_y_pred.append(model.predict([batch_data[0],label_list]))\n",
    "    test_y.append(batch_data[2])\n",
    "test_y_pred = np.concatenate(test_y_pred,axis=0)\n",
    "test_y = np.concatenate(test_y,axis=0)\n",
    "test_y_pred = np.array([[1 if i >= 0.5 else 0 for i in t] for t in test_y_pred])\n",
    "micro_f1 = f1_score(test_y_pred, test_y, average='micro')\n",
    "macro_f1 = f1_score(test_y_pred, test_y, average='macro')\n",
    "print('macro F1: %.4f' % macro_f1)\n",
    "print('micro F1: %.4f' % micro_f1)\n",
    "model.save(\"Model_save/best_model_capsule_label-%.4f.h5\" % (micro_f1))\n",
    "print(\"Saved model: Model_save/best_model_capsule_label-%.4f.h5\" % (micro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
